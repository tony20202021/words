# Перевод китайских слов с использованием LLM

Данный проект предназначен для создания кратких осмысленных переводов китайских слов на русский язык с использованием языковых моделей (LLM) из Hugging Face.

## Требования

- Python 3.8 или выше
- PyTorch 2.1 или выше
- Трансформеры от Hugging Face
- Для больших моделей: GPU с не менее 16 ГБ видеопамяти (для 7B моделей)

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/yourusername/chinese-translator.git
cd chinese-translator
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

3. (Опционально) Установите sacremoses для лучшей работы с некоторыми моделями:
```bash
pip install sacremoses
```

## Доступные модели

Проект поддерживает следующие модели:

### Легкие модели (1.5B-3B)
- **qwen2-1.5b**: Легкая версия Qwen2 (рекомендуется для слабых GPU)
- **bloom-3b**: Более легкая версия BLOOM
- **mgpt**: Многоязычная GPT модель (1.7B)
- **nllb-600m**: NLLB для перевода (600M)

### Средние модели (7B)
- **qwen2-7b**: Qwen2 7B от Alibaba (рекомендуется для китайского)
- **bloom-7b1**: BLOOM 7B от BigScience (хорошо для русского и китайского)
- **mistral-7b**: Mistral-7B-Instruct
- **falcon-7b**: Falcon от TII
- **bloomz-7b**: BloomZ 7B (инструктивная версия BLOOM)
- **wenzhong**: Wenzhong 3.5B (китайская модель)
- **baichuan-7b**: Baichuan2 7B (китайская модель)

### Русские модели
- **saiga2-7b**: Saiga2 (русскоязычная модель)
- **rugpt3-ai**: RuGPT3 от AI-Forever
- **rugpt3-sber**: RuGPT3 от Sberbank

### Специализированные модели перевода (seq2seq)
- **nllb-1.3b**: NLLB 1.3B для многоязычного перевода
- **m2m100**: M2M100 для перевода между многими языками
- **opus-mt-zh-en**: OpusMT для перевода с китайского на английский
- **mbart-50**: MBart для многоязычного перевода

### Тяжелые модели (8B+)
- **llama3-8b**: Llama 3 от Meta

## Использование

### Основной скрипт

```bash
python main_llm.py -i путь/к/файлу.json -o результат.json -m qwen2-1.5b
```

### Параметры

- `-i, --input`: Путь к входному JSON-файлу
- `-o, --output`: Путь к выходному JSON-файлу
- `-m, --model`: Название модели (по умолчанию: qwen2-1.5b)
- `-b, --batch`: Размер пакета для обработки (по умолчанию: 5)
- `--max`: Максимальное количество слов для обработки
- `--temp`: Температура генерации (по умолчанию: 0.3)
- `--cpu`: Использовать CPU вместо GPU
- `--list-models`: Показать список доступных моделей
- `--no-description`: Не использовать русское описание для перевода (только иероглиф)

### Новая функциональность

#### Режимы работы

1. **С описанием** (по умолчанию): Модель использует русское описание для создания перевода
2. **Без описания**: Модель переводит только на основе китайского иероглифа

```bash
# Режим с описанием
python main_llm.py -i data.json -m qwen2-1.5b

# Режим без описания
python main_llm.py -i data.json -m qwen2-1.5b --no-description
```

#### Автоматическое тестирование всех моделей

```bash
# Запуск всех доступных моделей в обоих режимах
bash run_models.sh
```

Этот скрипт автоматически:
- Определяет доступные модели
- Разделяет их на легкие и тяжелые
- Проверяет доступность GPU
- Запускает модели в обоих режимах
- Создает сравнительную таблицу результатов в форматах Markdown и CSV

### Тестирование моделей

Для сравнения результатов разных моделей:
```bash
python tests/test_llm.py -m qwen2-1.5b bloom-3b --show-desc
```

### Примеры использования

#### Базовый пример с легкой моделью
```bash
python main_llm.py -i data/examples/sample_words.json -m qwen2-1.5b
```

#### Обработка с более мощной моделью
```bash
python main_llm.py -i data/examples/sample_words.json -m bloom-7b1 --temp 0.4
```

#### Тестирование разных моделей
```bash
python tests/test_llm.py -m qwen2-1.5b bloom-3b mistral-7b
```

#### Использование собственной модели из Hugging Face
```bash
python main_llm.py -i data/examples/sample_words.json -m "your-org/your-model"
```

#### Перевод только на основе иероглифов
```bash
python main_llm.py -i data.json -m nllb-1.3b --no-description
```

## Производительность

- **Легкие модели (600M-3B)**: могут работать на GPU с 6-8 ГБ видеопамяти
- **Средние модели (7B)**: требуют GPU с не менее 16 ГБ видеопамяти
- **Тяжелые модели (>8B)**: требуют GPU с не менее 24 ГБ видеопамяти или многокарточную конфигурацию

## Структура проекта

```
repos/words/translation_summary/
│
├── src/                           # Исходные коды (модульная структура)
│   ├── __init__.py               # Инициализация пакета
│   ├── model_config.py           # Конфигурация моделей
│   ├── model_loader.py           # Загрузчик моделей
│   ├── prompt_generator.py       # Генератор промптов
│   ├── seq2seq_handler.py        # Обработчик seq2seq моделей
│   ├── llm_translator.py         # Основной класс переводчика
│   ├── chinese_translator_llm.py # Модуль для обработки JSON с LLM
│   └── generate_comparison_table.py # Генератор сравнительных таблиц
│
├── tests/                         # Тесты
│   ├── __init__.py               # Инициализация пакета тестов
│   └── test_llm.py               # Тестирование моделей
│
├── data/                          # Директория для данных
│   └── examples/                 # Примеры данных
│       └── sample_words.json     # Пример файла с китайскими словами
│
├── results/                       # Результаты выполнения
│   ├── run_YYYYMMDD_HHMMSS/      # Каталоги по дате/времени запуска
│   └── latest/                   # Символическая ссылка на последний запуск
│
├── main_llm.py                    # Основной скрипт запуска
├── run_models.sh                  # Скрипт для тестирования всех моделей
├── requirements.txt               # Зависимости
└── environment.yml                # Окружение conda
```

## Результаты

После выполнения создаются:
- **JSON файлы** с переводами для каждой модели и режима
- **Markdown таблица** со сравнением результатов
- **CSV файл** с полными данными для анализа
- **Лог файлы** с подробной информацией о выполнении

## Зависимости

Основные зависимости:
- torch>=2.1.0
- transformers>=4.36.0
- tqdm>=4.66.0
- tabulate>=0.9.0
- bitsandbytes>=0.40.0
- accelerate>=0.22.0
- sentencepiece
- protobuf
- einops
- numpy<2
- sacremoses

## Особенности

### Автоматическое управление памятью
- Квантизация больших моделей (8-bit)
- Автоматическое определение размера GPU
- Отключение квантизации для моделей с пользовательским кодом

### Поддержка различных типов моделей
- Каузальные языковые модели (GPT-style)
- Seq2seq модели перевода (NLLB, MBart, OpusMT)
- Специальная обработка для китайских моделей

### Кэширование и оптимизация
- Кэширование загруженных моделей
- Кэширование переводов
- Пакетная обработка данных
- Автоматическое обрезание длинных промптов

## Аутентификация Hugging Face

Для доступа к некоторым моделям может потребоваться токен Hugging Face:

```bash
huggingface-cli login
```

или установите переменную окружения:

```bash
export HUGGING_FACE_HUB_TOKEN=ваш_токен
```
