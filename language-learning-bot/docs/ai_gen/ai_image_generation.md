# AI Image Generation - –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–û–±–∑–æ—Ä AI —Å–∏—Å—Ç–µ–º—ã](#–æ–±–∑–æ—Ä-ai-—Å–∏—Å—Ç–µ–º—ã)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-ai-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤)
3. [Multi-ControlNet Pipeline](#multi-controlnet-pipeline)
4. [Conditioning –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã](#conditioning-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã)
5. [Prompt Engineering](#prompt-engineering)
6. [GPU —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#gpu-—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ-–∏-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
7. [–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞-–∏-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
8. [API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è](#api-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
9. [–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞](#–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥-–∏-–æ—Ç–ª–∞–¥–∫–∞)
10. [–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å-–∏-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
11. [Troubleshooting](#troubleshooting)

---

## –û–±–∑–æ—Ä AI —Å–∏—Å—Ç–µ–º—ã

Language Learning Bot –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é AI —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å–ª–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –Ω–∞ –±–∞–∑–µ **Stable Diffusion XL** —Å **Multi-ControlNet** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.

### üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤** - –æ—Ç –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–æ —ç–ª—å—Ñ–∏–π—Å–∫–æ–≥–æ
- **Multi-ControlNet –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** - –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 4 —Ç–∏–ø–æ–≤ conditioning
- **Intelligent Prompt Engineering** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
- **GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ –∂–µ–ª–µ–∑–æ
- **Lazy loading** - –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
- **Production ready** - health monitoring, performance tracking

### üèóÔ∏è –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫:
```
ü§ñ AI Models:
‚îú‚îÄ‚îÄ Stable Diffusion XL Base 1.0        # –û—Å–Ω–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
‚îú‚îÄ‚îÄ ControlNet Canny SDXL 1.0           # Edge detection conditioning
‚îú‚îÄ‚îÄ ControlNet Depth SDXL 1.0           # Depth estimation conditioning
‚îú‚îÄ‚îÄ ControlNet Segmentation SDXL 1.0    # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è conditioning
‚îú‚îÄ‚îÄ ControlNet Scribble SDXL 1.0        # Scribble conditioning

üìö AI Frameworks:
‚îú‚îÄ‚îÄ Diffusers >= 0.24.0                 # HuggingFace Diffusers
‚îú‚îÄ‚îÄ Transformers >= 4.35.0              # HuggingFace Transformers
‚îú‚îÄ‚îÄ Accelerate >= 0.24.0                # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
‚îú‚îÄ‚îÄ XFormers >= 0.0.22                  # Attention optimization
‚îú‚îÄ‚îÄ ControlNet-Aux >= 0.0.9             # ControlNet utilities
‚îú‚îÄ‚îÄ Triton >= 2.1.0                     # CUDA kernels
‚îî‚îÄ‚îÄ PyTorch >= 2.1.0                    # –û—Å–Ω–æ–≤–Ω–æ–π ML framework

üîß Infrastructure:
‚îú‚îÄ‚îÄ CUDA 11.8                           # GPU computation
‚îú‚îÄ‚îÄ FastAPI                             # Web framework
‚îú‚îÄ‚îÄ Hydra                               # Configuration management
‚îú‚îÄ‚îÄ Pydantic                            # Data validation
‚îî‚îÄ‚îÄ AsyncIO                             # Asynchronous processing
```

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### üéØ –û–±—â–∞—è —Å—Ö–µ–º–∞ AI pipeline:

```mermaid
graph TB
    A[Character Input] --> B[Character Preprocessing]
    B --> C[Base Image Rendering]
    
    C --> D[Multi-Conditioning Generation]
    D --> E[Canny Edge Detection]
    D --> F[Depth Estimation]
    D --> G[Segmentation]
    D --> H[Scribble Generation]
    
    I[Prompt Builder] --> J[Style Definitions]
    J --> K[Semantic Analysis]
    K --> L[Text Prompt]
    
    E --> M[Multi-ControlNet Pipeline]
    F --> M
    G --> M
    H --> M
    L --> M
    
    M --> N[Stable Diffusion XL]
    N --> O[GPU Manager]
    O --> P[Generated Image]
    
    style A fill:#e1f5fe
    style M fill:#f3e5f5
    style N fill:#fff3e0
    style P fill:#e8f5e8
```

### üèóÔ∏è –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Writing Service:

#### 1. **AI Image Generator** (`ai_image_generator.py`)
–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π orchestrator –≤—Å–µ–≥–æ AI pipeline:
```python
class AIImageGenerator:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∏–µ—Ä–æ–≥–ª–∏—Ñ–∞–º"""
    
    async def generate_character_image(
        self,
        character: str,
        translation: str = "",
        conditioning_weights: Optional[Dict[str, float]] = None,
        seed: Optional[int] = None,
        **generation_params
    ) -> AIGenerationResult
```

**Workflow –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:**
1. **Lazy model loading** - –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏
2. **Character preprocessing** - —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∏–µ—Ä–æ–≥–ª–∏—Ñ–∞ –≤ –±–∞–∑–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
3. **Multi-conditioning** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ conditioning
4. **Prompt building** - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ intelligent –ø—Ä–æ–º–ø—Ç–∞
5. **AI generation** - Multi-ControlNet + Stable Diffusion XL
6. **Result packaging** - —É–ø–∞–∫–æ–≤–∫–∞ –≤ base64 + metadata

#### 2. **Model Loader** (`model_loader.py`)
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ lifecycle AI –º–æ–¥–µ–ª–µ–π:
```python
class ModelLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ AI –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    async def load_stable_diffusion_xl(self, model_name: str)
    async def load_controlnet_models(self, model_configs: Dict[str, str])
    async def load_auxiliary_models(self)
    async def setup_multi_controlnet_pipeline(self, controlnets: Dict)
```

**Features:**
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏** –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç GPU
- **Memory tracking** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
- **Error recovery** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏
- **Performance metrics** - –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏

#### 3. **GPU Manager** (`gpu_manager.py`)
Intelligent —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ GPU —Ä–µ—Å—É—Ä—Å–∞–º–∏:
```python
class GPUManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä GPU –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ AI –º–æ–¥–µ–ª–µ–π"""
    
    def get_gpu_status(self) -> GPUStatus
    def optimize_pipeline(self, pipeline) -> Dict[str, bool]
    def get_recommended_batch_size(self, image_size: int) -> int
```

**Optimization Profiles:**
```python
# Ultra High Memory (80GB+)
OptimizationProfile(
    memory_efficient=False,
    attention_slicing=False,
    max_batch_size=8,
    torch_compile=True
)

# High Memory (40-80GB)
OptimizationProfile(
    memory_efficient=False,
    vae_tiling=True,
    max_batch_size=4,
    torch_compile=True
)

# Medium Memory (24-40GB)
OptimizationProfile(
    memory_efficient=True,
    attention_slicing=True,
    vae_slicing=True,
    max_batch_size=2
)

# Low Memory (12-24GB)
OptimizationProfile(
    memory_efficient=True,
    cpu_offload=True,
    max_batch_size=1
)
```

---

## Multi-ControlNet Pipeline

### üéõÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Multi-ControlNet:

Multi-ControlNet Pipeline –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç **4 —Ç–∏–ø–∞ conditioning** –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π:

```python
class MultiControlNetPipeline:
    """Multi-ControlNet pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º conditioning"""
    
    async def generate(
        self,
        prompt: str,
        control_images: Dict[str, Image.Image] = None,
        conditioning_scales: Dict[str, float] = None,
        **generation_params
    ) -> Image.Image
```

### üé® –¢–∏–ø—ã ControlNet –∏ –∏—Ö –≤–µ—Å–∞:

#### **Style-based Conditioning Weights:**
```yaml
comic_style:
  canny: 0.9        # –°–∏–ª—å–Ω—ã–µ –∫–æ–Ω—Ç—É—Ä—ã –¥–ª—è –∫–æ–º–∏–∫—Å–æ–≤
  depth: 0.5        # –£–º–µ—Ä–µ–Ω–Ω–∞—è –æ–±—ä–µ–º–Ω–æ—Å—Ç—å
  segmentation: 0.7 # –ß–µ—Ç–∫–∏–µ —Ü–≤–µ—Ç–æ–≤—ã–µ –∑–æ–Ω—ã
  scribble: 0.3     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å

watercolor_style:
  canny: 0.4        # –†–∞–∑–º—ã—Ç—ã–µ –∫–æ–Ω—Ç—É—Ä—ã
  depth: 0.3        # –ú—è–≥–∫–∞—è –æ–±—ä–µ–º–Ω–æ—Å—Ç—å
  segmentation: 0.3 # –ü–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
  scribble: 0.8     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–≤–æ–±–æ–¥–∞

realistic_style:
  canny: 0.8        # –¢–æ—á–Ω—ã–µ –∫–æ–Ω—Ç—É—Ä—ã
  depth: 0.9        # –°–∏–ª—å–Ω–∞—è –æ–±—ä–µ–º–Ω–æ—Å—Ç—å
  segmentation: 0.6 # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–≤–µ—Ç–∞
  scribble: 0.2     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è
```

### ‚ö° Pipeline Optimizations:

#### **GPU Memory Optimizations:**
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç GPU –ø–∞–º—è—Ç–∏
if gpu_memory >= 80:  # 80GB+
    pipeline_config = PipelineConfig(
        memory_efficient=False,
        enable_attention_slicing=False,
        max_batch_size=8,
        use_torch_compile=True
    )
elif gpu_memory >= 24:  # 24-80GB
    pipeline_config = PipelineConfig(
        memory_efficient=True,
        enable_attention_slicing=True,
        enable_vae_slicing=True,
        max_batch_size=2
    )
else:  # <24GB
    pipeline_config = PipelineConfig(
        memory_efficient=True,
        enable_cpu_offload=True,
        enable_sequential_cpu_offload=True,
        max_batch_size=1
    )
```

#### **Performance Optimizations:**
- **XFormers** - memory efficient attention
- **Torch Compile** - JIT compilation –¥–ª—è UNet
- **Channels Last** - memory format optimization
- **VAE Optimizations** - slicing –∏ tiling
- **CPU Offload** - –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ GPU –ø–∞–º—è—Ç–∏

---

## Conditioning –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã

### üñºÔ∏è –ß–µ—Ç—ã—Ä–µ —Ç–∏–ø–∞ Conditioning:

#### 1. **Canny Edge Detection** (`canny_conditioning.py`)
–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç—É—Ä–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è:

```python
class CannyConditioning:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä Canny edge conditioning –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    available_methods = [
        "opencv_canny",              # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π OpenCV Canny
        "structured_edge_detection", # Structured edges
        "multi_scale_canny",         # Multi-scale –ø–æ–¥—Ö–æ–¥
        "adaptive_canny"             # Adaptive thresholding
    ]
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
- **–ò–µ—Ä–æ–≥–ª–∏—Ñ—ã** - —á–µ—Ç–∫–∏–µ –∫–æ–Ω—Ç—É—Ä—ã —à—Ç—Ä–∏—Ö–æ–≤
- **–õ–∞—Ç–∏–Ω–∏—Ü–∞** - outline –±—É–∫–≤
- **–ê—Ä–∞–±—Å–∫–∞—è –≤—è–∑—å** - –∫–æ–Ω—Ç—É—Ä—ã –∫–∞–ª–ª–∏–≥—Ä–∞—Ñ–∏–∏

#### 2. **Depth Estimation** (`depth_conditioning.py`)
–°–æ–∑–¥–∞–Ω–∏–µ depth maps –¥–ª—è –æ–±—ä–µ–º–Ω–æ—Å—Ç–∏:

```python
class DepthConditioning:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä depth conditioning –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    available_methods = [
        "black_and_white_depth",     # Simple depth from intensity
        "stroke_thickness_depth",    # Depth based on stroke width
        "distance_transform_depth",  # Distance transform approach
        "morphological_depth",       # Morphological operations
        "multi_layer_depth"          # Layered depth construction
    ]
```

**Features:**
- **MiDaS integration** - AI-powered depth estimation
- **Stroke analysis** - –≥–ª—É–±–∏–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—â–∏–Ω—ã –ª–∏–Ω–∏–π
- **Multi-layer** - —Å–ª–æ–∂–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è –≥–ª—É–±–∏–Ω—ã

#### 3. **Segmentation** (`segmentation_conditioning.py`)
–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è:

```python
class SegmentationConditioning:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä segmentation conditioning –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    available_methods = [
        "black_and_white_segmentation", # Simple B&W segmentation
        "radical_segmentation",          # Kangxi radical-based
        "stroke_type_segmentation",     # –ü–æ —Ç–∏–ø–∞–º —à—Ç—Ä–∏—Ö–æ–≤
        "color_based_segmentation"       # K-means clustering
    ]
```

**Advanced Features:**
- **Radical-based** - —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ radicals Kangxi
- **SAM integration** - Segment Anything Model
- **Stroke typing** - —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º —à—Ç—Ä–∏—Ö–æ–≤ (–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ, –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ)

#### 4. **Scribble Generation** (`scribble_conditioning.py`)
–°–æ–∑–¥–∞–Ω–∏–µ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞–±—Ä–æ—Å–∫–æ–≤:

```python
class ScribbleConditioning:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä scribble conditioning –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    available_methods = [
        "skeletonization_scribble",      # Zhang-Suen skeletonization
        "morphological_simplification",  # Morphological thinning
        "vectorization_simplification",  # Vector-based approach
        "hand_drawn_simulation",         # Simulate hand drawing
        "multi_level_abstraction",       # Multiple abstraction levels
        "style_aware_scribble"           # Style-specific scribbles
    ]
```

**Artistic Features:**
- **Hand-drawn simulation** - –∏–º–∏—Ç–∞—Ü–∏—è —Ä–∏—Å–æ–≤–∞–Ω–∏—è –æ—Ç —Ä—É–∫–∏
- **Multi-level abstraction** - —Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
- **Style awareness** - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å

### üîÑ Parallel Conditioning Generation:

```python
async def _generate_all_conditioning(
    self,
    base_image: Image.Image,
    character: str,
) -> Dict[str, Dict[str, Image.Image]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–∏–ø—ã conditioning –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    
    # –°–æ–∑–¥–∞–µ–º async –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
    tasks = []
    for conditioning_type, generator in self.conditioning_generators.items():
        method = random.choice(generator.get_available_methods())
        task = asyncio.create_task(
            generator.generate_from_image(base_image, method=method)
        )
        tasks.append((conditioning_type, method, task))
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö conditioning
    conditioning_images = {}
    for conditioning_type, method, task in tasks:
        result = await task
        if result.success:
            conditioning_images[conditioning_type] = {method: result.image}
    
    return conditioning_images
```

---

## Prompt Engineering

### üé® Style-Aware Prompt Building:

#### **Style Definitions** (`style_definitions.py`)
Comprehensive —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–∏–ª–µ–≤—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π:

```python
class StyleDefinitions:
    """–°–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    styles = {
        "comic": StyleDefinition(
            name="comic",
            display_name="Comic Book",
            base_template="A vibrant comic book style illustration of {meaning}, inspired by the Chinese character {character}, featuring bold outlines and dynamic composition",
            recommended_steps=25,
            recommended_cfg=8.0,
            controlnet_weights={"canny": 0.9, "depth": 0.5, "segmentation": 0.7, "scribble": 0.3}
        ),
        
        "watercolor": StyleDefinition(
            name="watercolor",
            display_name="Watercolor Painting",
            base_template="A soft watercolor painting depicting {meaning}, with flowing brushstrokes inspired by the Chinese character {character}, featuring delicate washes and artistic spontaneity",
            recommended_steps=35,
            recommended_cfg=6.5,
            controlnet_weights={"canny": 0.4, "depth": 0.3, "segmentation": 0.3, "scribble": 0.8}
        ),
        
        "ink": StyleDefinition(
            name="ink",
            display_name="Chinese Ink Painting",
            base_template="A traditional Chinese ink painting of {meaning}, capturing the spirit of the character {character}, with flowing brushstrokes and philosophical depth",
            recommended_steps=32,
            recommended_cfg=6.0,
            controlnet_weights={"canny": 0.6, "depth": 0.3, "segmentation": 0.2, "scribble": 0.9}
        )
    }
```

#### **Intelligent Prompt Builder** (`prompt_builder.py`)
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:

```python
class PromptBuilder:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    async def build_prompt(
        self,
        character: str,
        translation: str = "",
        style: str = "comic",
    ) -> PromptResult:
        """–°—Ç—Ä–æ–∏—Ç intelligent –ø—Ä–æ–º–ø—Ç –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        
        # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∏–ª–µ–≤—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        style_data = self.style_definitions.get_style_definition(style)
        
        # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        main_prompt = await self._build_main_prompt(
            character=character,
            translation=translation,
            style=style,
            style_data=style_data,
        )
        
        # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞
        main_prompt = self._optimize_prompt_length(main_prompt, self.config.max_prompt_length)
        
        return PromptResult(
            success=True,
            main_prompt=main_prompt,
            style_data=style_data
        )
```

## GPU —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### üéÆ Intelligent GPU Management:

#### **Hardware Detection:**
```python
def _detect_gpu_info(self) -> Dict[str, Any]:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ GPU"""
    
    device_props = torch.cuda.get_device_properties(0)
    
    gpu_info = {
        "name": device_props.name,
        "total_memory_gb": device_props.total_memory / 1024**3,
        "compute_capability": (device_props.major, device_props.minor),
        "multi_processor_count": device_props.multi_processor_count,
    }
    
    # Extended info —á–µ—Ä–µ–∑ pynvml
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        gpu_info["temperature_celsius"] = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
        gpu_info["power_usage_watts"] = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0
    except:
        pass
    
    return gpu_info
```

#### **Automatic Optimization Profiles:**
```python
def _determine_optimization_profile(self) -> OptimizationProfile:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø—Ä–æ—Ñ–∏–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    total_memory = self.gpu_info["total_memory_gb"]
    
    if total_memory >= 80:
        return OptimizationProfile(
            name="ultra_high_memory",
            memory_efficient=False,
            attention_slicing=False,
            vae_slicing=False,
            cpu_offload=False,
            max_batch_size=8,
            enable_xformers=True
        )
    elif total_memory >= 40:
        return OptimizationProfile(
            name="high_memory",
            memory_efficient=False,
            vae_tiling=True,
            max_batch_size=4,
            enable_xformers=True
        )
    elif total_memory >= 24:
        return OptimizationProfile(
            name="medium_memory",
            memory_efficient=True,
            attention_slicing=True,
            vae_slicing=True,
            max_batch_size=2,
            enable_xformers=True
        )
    else:
        return OptimizationProfile(
            name="minimal_memory",
            memory_efficient=True,
            attention_slicing=True,
            cpu_offload=True,
            sequential_cpu_offload=True,
            max_batch_size=1,
            enable_xformers=False
        )
```

#### **Memory Monitoring:**
```python
@contextmanager
def memory_monitor(self, operation_name: str = "operation"):
    """Context manager –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    
    start_time = time.time()
    memory_before = self.get_memory_usage()
    
    try:
        yield
    finally:
        end_time = time.time()
        memory_after = self.get_memory_usage()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        operation_time = end_time - start_time
        memory_diff = memory_after.get("used_gb", 0) - memory_before.get("used_gb", 0)
        
        self.performance_history.append({
            "timestamp": start_time,
            "operation": operation_name,
            "duration_seconds": operation_time,
            "memory_diff_gb": memory_diff,
            "peak_memory_gb": memory_after.get("used_gb", 0)
        })
```

---

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### üîß Hardware Requirements:

#### **Minimum (12GB GPU):**
```
GPU: RTX 3080, RTX 4070 Ti, A4000
RAM: 32GB System RAM
Storage: 100GB+ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∏ cache
CUDA: 11.8+
```

#### **Recommended (24GB+ GPU):**
```
GPU: RTX 3090, RTX 4090, A5000, A6000
RAM: 64GB System RAM
Storage: 500GB+ NVMe SSD
CUDA: 11.8+
```

#### **Optimal (80GB+ GPU):**
```
GPU: A100, H100
RAM: 128GB+ System RAM
Storage: 1TB+ NVMe SSD
CUDA: 11.8+
```

### üì¶ Installation:

#### **1. GPU Environment Setup:**
```bash
# –°–æ–∑–¥–∞–Ω–∏–µ AI –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
conda env create -f writing_service/environment_gpu.yml
conda activate amikhalev_writing_images_service

# –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name()}')"
```

#### **2. AI Dependencies:**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ AI –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r writing_service/requirements_gpu.txt

# –ü—Ä–æ–≤–µ—Ä–∫–∞ XFormers
python -c "import xformers; print(f'XFormers: {xformers.__version__}')"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Diffusers
python -c "from diffusers import StableDiffusionXLPipeline; print('Diffusers OK')"
```

#### **3. Cache Directories:**
```bash
# –°–æ–∑–¥–∞–Ω–∏–µ cache –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è AI –º–æ–¥–µ–ª–µ–π
mkdir -p writing_service/cache/{huggingface,transformers,datasets,torch,pytorch_kernel_cache}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
export HF_HOME="./writing_service/cache/huggingface"
export TORCH_HOME="./writing_service/cache/torch"
export TRANSFORMERS_CACHE="./writing_service/cache/transformers"
```

### ‚öôÔ∏è Configuration:

#### **AI Generation Config** (`writing_service/conf/config/ai_generation.yaml`):
```yaml
ai_generation:
  enabled: true
  
  # AI Models
  models:
    base_model: "stabilityai/stable-diffusion-xl-base-1.0"
    controlnet_models:
      canny: "diffusers/controlnet-canny-sdxl-1.0"
      depth: "diffusers/controlnet-depth-sdxl-1.0"
      segmentation: "diffusers/controlnet-seg-sdxl-1.0"
      scribble: "diffusers/controlnet-scribble-sdxl-1.0"
  
  # Generation Parameters
  generation:
    num_inference_steps: 30
    guidance_scale: 7.5
    width: 1024
    height: 1024
    batch_size: 1
  
  # GPU Settings
  gpu:
    device: "cuda"
    memory_efficient: true
    enable_attention_slicing: true
    max_batch_size: 4
    use_torch_compile: true
```

---

## API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### üåê Writing Service API:

#### **Health Checks:**
```bash
# –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:8600/health

# –î–µ—Ç–∞–ª—å–Ω–∞—è AI –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
curl http://localhost:8600/health/detailed

# –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
curl http://localhost:8600/health/ready
```

#### **AI Generation:**
```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
curl -X POST http://localhost:8600/api/writing/generate-writing-image \
  -H "Content-Type: application/json" \
  -d '{
    "word": "Â≠¶‰π†",
    "translation": "study",
    "language": "chinese",
    "style": "traditional",
    "width": 1024,
    "height": 1024,
    "include_conditioning_images": true,
    "include_prompt": true,
    "seed": 12345
  }'
```

#### **Response Format:**
```json
{
  "success": true,
  "status": "SUCCESS",
  "generated_image_base64": "iVBORw0KGgoAAAANS...",
  "base_image_base64": "iVBORw0KGgoAAAANS...",
  "conditioning_images_base64": {
    "canny": {"opencv_canny": "iVBORw0KGgoAAAANS..."},
    "depth": {"ai_depth_estimation": "iVBORw0KGgoAAAANS..."},
    "segmentation": {"ai_segmentation": "iVBORw0KGgoAAAANS..."},
    "scribble": {"skeletonization_scribble": "iVBORw0KGgoAAAANS..."}
  },
  "prompt_used": "A traditional Chinese calligraphy illustration of study...",
  "generation_metadata": {
    "character": "Â≠¶‰π†",
    "translation": "study",
    "generation_time_ms": 8500,
    "seed_used": 12345,
    "model_used": "stabilityai/stable-diffusion-xl-base-1.0",
    "conditioning_methods_used": {
      "canny": ["opencv_canny"],
      "depth": ["ai_depth_estimation"],
      "segmentation": ["ai_segmentation"],
      "scribble": ["skeletonization_scribble"]
    }
  }
}
```

### üîó Frontend Integration:

#### **API Client** (`frontend/app/api/client.py`):
```python
class APIClient:
    """API client —Å Writing Service –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""
    
    async def generate_writing_image(
        self,
        word: str,
        translation: str = "",
        language: str = "auto",
        style: str = "traditional",
        **kwargs
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Writing Service"""
        
        request_data = {
            "word": word,
            "translation": translation,
            "language": language,
            "style": style,
            **kwargs
        }
        
        try:
            response = await self._make_request(
                "POST",
                f"{self.writing_service_url}/api/writing/generate-writing-image",
                json=request_data,
                timeout=30
            )
            return response
        except Exception as e:
            logger.error(f"Writing image generation failed: {e}")
            raise
```

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞

### üìä Performance Monitoring:

#### **AI Status Dashboard:**
```bash
# –°—Ç–∞—Ç—É—Å AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
curl http://localhost:8600/api/writing/status | jq '.'

# GPU utilization
curl http://localhost:8600/health/detailed | jq '.gpu_status'

# AI generation statistics
curl http://localhost:8600/health/detailed | jq '.service_status.ai_status'
```

#### **Memory Tracking:**
```python
# –í GPU Manager
def get_diagnostics(self) -> Dict[str, Any]:
    """–ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
    
    return {
        "gpu_info": self.gpu_info,
        "optimization_profile": {
            "name": self.optimization_profile.name,
            "settings": {...}
        },
        "current_status": self.get_gpu_status().__dict__,
        "memory_history": self.memory_history[-10:],
        "performance_history": self.performance_history[-10:],
        "recommendations": self.get_optimization_recommendations()
    }
```

### üîç Debugging:

#### **Logging Configuration:**
```yaml
# writing_service/conf/config/logging.yaml
logging:
  level: "DEBUG"  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ AI
  loggers:
    generation:
      level: "INFO"
      include_timing: true
      include_parameters: true
    
    ai_models:
      level: "DEBUG"
      include_model_loading: true
      include_memory_usage: true
```

#### **Debug Commands:**
```bash
# –î–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
tail -f writing_service/logs/writing_service.log | grep "AI"

# GPU memory monitoring
watch -n 1 'nvidia-smi'

# Model loading progress
tail -f writing_service/logs/writing_service.log | grep "Loading"
```

---

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### ‚ö° Performance Benchmarks:

#### **Generation Times by GPU:**
```
RTX 4090 (24GB):
‚îú‚îÄ‚îÄ Single image (1024x1024): ~8-12 seconds
‚îú‚îÄ‚îÄ Batch 2: ~14-18 seconds
‚îî‚îÄ‚îÄ Batch 4: ~25-35 seconds

A100 (80GB):
‚îú‚îÄ‚îÄ Single image: ~6-8 seconds
‚îú‚îÄ‚îÄ Batch 4: ~18-24 seconds
‚îî‚îÄ‚îÄ Batch 8: ~32-45 seconds

RTX 3080 (12GB):
‚îú‚îÄ‚îÄ Single image: ~12-18 seconds
‚îú‚îÄ‚îÄ Batch 2: ~22-30 seconds
‚îî‚îÄ‚îÄ Memory optimizations required
```

#### **Optimization Strategies:**
```python
# 1. Model Caching
class ModelCache:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.cached_models = {}
        self.cache_timestamps = {}
    
    async def get_or_load_model(self, model_key: str):
        if model_key in self.cached_models:
            return self.cached_models[model_key]
        
        # Load and cache
        model = await self._load_model(model_key)
        self.cached_models[model_key] = model
        self.cache_timestamps[model_key] = time.time()
        return model

# 2. Batch Processing
class BatchProcessor:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    
    async def process_batch(self, requests: List[GenerationRequest]) -> List[GenerationResult]:
        # Group by similar parameters
        batches = self._group_requests(requests)
        
        results = []
        for batch in batches:
            batch_result = await self._process_single_batch(batch)
            results.extend(batch_result)
        
        return results

# 3. Memory Management
async def _optimize_memory_usage(self):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏"""
    
    # Clear unused models
    await self._cleanup_unused_models()
    
    # Optimize pipeline
    if hasattr(self.pipeline, 'enable_vae_slicing'):
        self.pipeline.enable_vae_slicing()
    
    # Clear GPU cache
    torch.cuda.empty_cache()
    gc.collect()
```

### üéØ Advanced Optimizations:

#### **Intelligent Batching:**
```python
class IntelligentBatcher:
    """–£–º–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def group_generation_requests(self, requests: List[GenerationRequest]) -> List[List[GenerationRequest]]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"""
        
        batches = []
        current_batch = []
        
        for request in requests:
            if self._can_batch_together(current_batch, request):
                current_batch.append(request)
            else:
                if current_batch:
                    batches.append(current_batch)
                current_batch = [request]
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä batch
            if len(current_batch) >= self.max_batch_size:
                batches.append(current_batch)
                current_batch = []
        
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _can_batch_together(self, batch: List[GenerationRequest], new_request: GenerationRequest) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è batching"""
        
        if not batch:
            return True
        
        reference = batch[0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        compatible = (
            reference.width == new_request.width and
            reference.height == new_request.height and
            reference.num_inference_steps == new_request.num_inference_steps and
            reference.guidance_scale == new_request.guidance_scale and
            reference.style == new_request.style
        )
        
        return compatible
```

#### **Model Warming:**
```python
class ModelWarmer:
    """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞"""
    
    async def warmup_models(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≥—Ä–µ–≤–∞–µ—Ç –≤—Å–µ AI –º–æ–¥–µ–ª–∏"""
        
        warmup_results = {}
        
        # Warmup characters –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_characters = ["Êµã", "ËØï", "È™å", "Â≠¶", "‰π†"]
        
        for i, char in enumerate(test_characters):
            start_time = time.time()
            
            try:
                result = await self.ai_generator.generate_character_image(
                    character=char,
                    translation=f"warmup_{i}",
                    include_conditioning_images=False,
                    include_prompt=False
                )
                
                warmup_time = time.time() - start_time
                warmup_results[char] = {
                    "success": result.success,
                    "time_ms": int(warmup_time * 1000),
                    "memory_usage": self.gpu_manager.get_memory_usage()
                }
                
                logger.info(f"Warmup {char}: {warmup_time:.2f}s")
                
            except Exception as e:
                warmup_results[char] = {
                    "success": False,
                    "error": str(e),
                    "time_ms": int((time.time() - start_time) * 1000)
                }
        
        return warmup_results
```

---

## Troubleshooting

### üêõ Common Issues and Solutions:

#### **1. CUDA Out of Memory:**
```bash
# –°–∏–º–ø—Ç–æ–º—ã
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB

# –†–µ—à–µ–Ω–∏—è
# 1. –£–º–µ–Ω—å—à–∏—Ç—å batch size
echo "generation.batch_size: 1" >> writing_service/conf/config/ai_generation.yaml

# 2. –í–∫–ª—é—á–∏—Ç—å memory optimizations
echo "gpu.memory_efficient: true" >> writing_service/conf/config/ai_generation.yaml
echo "gpu.enable_attention_slicing: true" >> writing_service/conf/config/ai_generation.yaml
echo "gpu.enable_cpu_offload: true" >> writing_service/conf/config/ai_generation.yaml

# 3. –û—á–∏—Å—Ç–∏—Ç—å GPU cache
python -c "import torch; torch.cuda.empty_cache()"
```

#### **2. Model Loading Failures:**
```bash
# –°–∏–º–ø—Ç–æ–º—ã
OSError: Can't load tokenizer for 'stabilityai/stable-diffusion-xl-base-1.0'

# –†–µ—à–µ–Ω–∏—è
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–µ—Ç–µ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
curl -I https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0

# 2. –û—á–∏—Å—Ç–∏—Ç—å HuggingFace cache
rm -rf writing_service/cache/huggingface/*

# 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å git-lfs
git lfs install

# 4. –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
python -c "
from diffusers import StableDiffusionXLPipeline
pipeline = StableDiffusionXLPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0')
print('Model downloaded successfully')
"
```

#### **3. XFormers Issues:**
```bash
# –°–∏–º–ø—Ç–æ–º—ã
ImportError: No module named 'xformers'

# –†–µ—à–µ–Ω–∏—è
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å XFormers –¥–ª—è –≤–∞—à–µ–π CUDA –≤–µ—Ä—Å–∏–∏
pip install xformers --index-url https://download.pytorch.org/whl/cu118

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å PyTorch –∏ XFormers
python -c "
import torch
import xformers
print(f'PyTorch: {torch.__version__}')
print(f'XFormers: {xformers.__version__}')
print(f'CUDA: {torch.version.cuda}')
"

# 3. –û—Ç–∫–ª—é—á–∏—Ç—å XFormers –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç—Å—è
echo "gpu.enable_xformers: false" >> writing_service/conf/config/ai_generation.yaml
```

#### **4. Slow Generation Times:**
```bash
# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU utilization
nvidia-smi -l 1

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
curl http://localhost:8600/health/detailed | jq '.gpu_diagnostics.optimization_profile'

# 3. –í–∫–ª—é—á–∏—Ç—å torch.compile (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
echo "gpu.use_torch_compile: true" >> writing_service/conf/config/ai_generation.yaml

# 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É GPU
curl http://localhost:8600/health/detailed | jq '.gpu_status.temperature_celsius'
```

### üîß Performance Debugging:

#### **Profiling AI Generation:**
```python
import time
import torch.profiler

class AIProfiler:
    """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.generation_times = []
        self.memory_usage = []
    
    async def profile_generation(self, character: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        
        profiling_results = {}
        
        with torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            
            start_time = time.time()
            memory_before = torch.cuda.memory_allocated()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            result = await self.ai_generator.generate_character_image(character)
            
            end_time = time.time()
            memory_after = torch.cuda.memory_allocated()
            
            profiling_results = {
                "character": character,
                "success": result.success,
                "total_time_ms": int((end_time - start_time) * 1000),
                "memory_used_mb": (memory_after - memory_before) / 1024**2,
                "peak_memory_mb": torch.cuda.max_memory_allocated() / 1024**2
            }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å
        prof.export_chrome_trace(f"ai_profile_{character}_{int(time.time())}.json")
        
        return profiling_results
```

#### **Memory Leak Detection:**
```python
class MemoryLeakDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —É—Ç–µ—á–µ–∫ GPU –ø–∞–º—è—Ç–∏"""
    
    def __init__(self):
        self.baseline_memory = None
        self.generation_count = 0
    
    def start_monitoring(self):
        """–ù–∞—á–∏–Ω–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É—Ç–µ—á–µ–∫"""
        torch.cuda.empty_cache()
        self.baseline_memory = torch.cuda.memory_allocated()
        self.generation_count = 0
        logger.info(f"Memory monitoring started. Baseline: {self.baseline_memory / 1024**2:.1f}MB")
    
    def check_for_leaks(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏"""
        
        current_memory = torch.cuda.memory_allocated()
        memory_diff = current_memory - self.baseline_memory
        
        leak_detected = memory_diff > 100 * 1024**2  # 100MB threshold
        
        return {
            "leak_detected": leak_detected,
            "baseline_memory_mb": self.baseline_memory / 1024**2,
            "current_memory_mb": current_memory / 1024**2,
            "memory_diff_mb": memory_diff / 1024**2,
            "generation_count": self.generation_count,
            "memory_per_generation_mb": memory_diff / max(1, self.generation_count) / 1024**2
        }
    
    def after_generation(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        self.generation_count += 1
        
        if self.generation_count % 10 == 0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
            leak_info = self.check_for_leaks()
            
            if leak_info["leak_detected"]:
                logger.warning(f"Memory leak detected! "
                             f"Diff: {leak_info['memory_diff_mb']:.1f}MB "
                             f"after {leak_info['generation_count']} generations")
                
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—á–∏—Å—Ç–∫–∏
                torch.cuda.empty_cache()
```

### üìà Performance Optimization Checklist:

#### **Monitoring in Production:**
```bash
# Continuous monitoring scripts
#!/bin/bash

# Monitor AI service health
while true; do
    echo "$(date): Checking AI service health..."
    
    # Health check
    health_status=$(curl -s http://localhost:8600/health | jq -r '.status')
    echo "Health: $health_status"
    
    # GPU memory
    gpu_memory=$(curl -s http://localhost:8600/health/detailed | jq -r '.gpu_status.memory.percent_used')
    echo "GPU Memory: $gpu_memory%"
    
    # Generation count
    gen_count=$(curl -s http://localhost:8600/api/writing/status | jq -r '.result.total_generations')
    echo "Total generations: $gen_count"
    
    echo "---"
    sleep 60
done
```

---
