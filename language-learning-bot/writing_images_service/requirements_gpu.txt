# ==================================================  
# AI REQUIREMENTS FOR WRITING SERVICE - REAL GENERATION + TRANSLATION
# writing_service/requirements_gpu.txt
# ОБНОВЛЕНО: Добавлены зависимости для Translation Service с поддержкой Qwen
# ==================================================

# ============ CORE AI FRAMEWORKS ============
# HuggingFace ecosystem - КРИТИЧНО для AI генерации и перевода
transformers>=4.39.3,<4.45.0  # ОБНОВЛЕНО: расширен диапазон для совместимости с Qwen
diffusers==0.25.0
accelerate>=0.24.0
safetensors>=0.4.0
tokenizers>=0.14.0
# Hugging Face Hub для загрузки моделей
huggingface_hub>=0.23.0

# ============ TRANSLATION SPECIFIC LIBRARIES ============
# НОВЫЙ РАЗДЕЛ: Специальные библиотеки для Translation Service

# Qwen модели - поддержка Trust Remote Code
# (Qwen требует trust_remote_code=True для некоторых компонентов)
torch>=2.1.0  # Базовый PyTorch для всех моделей

# NLLB специфичные зависимости
# NLLB модели используют специальные токенизаторы
sentencepiece>=0.1.99  # Для NLLB и некоторых других multilingual моделей
protobuf>=3.20.0       # Для работы с sentencepiece

# mT5 специфичные зависимости  
# mT5 может использовать специальные компоненты
# (базовые transformers уже включают поддержку T5)

# OPUS модели
# OPUS модели обычно работают через стандартные transformers
# Дополнительных зависимостей не требуют

# Multilingual поддержка
regex>=2023.6.3        # Для advanced text processing в многоязычных моделях
langdetect>=1.0.9      # Автоопределение языка (для fallback логики)

# ============ DEEP LEARNING & COMPUTER VISION ============
# ControlNet и auxiliary models - ОБЯЗАТЕЛЬНО
controlnet-aux>=0.0.10

# XFormers для оптимизации attention - КРИТИЧНО для производительности
xformers>=0.0.22

# OpenCV для image processing - ОБЯЗАТЕЛЬНО для conditioning
opencv-python>=4.8.0
opencv-contrib-python>=4.8.0

# Дополнительные CV библиотеки
scikit-image>=0.21.0
imageio>=2.31.0

# ============ WEB FRAMEWORK ============
# FIXED: FastAPI/Pydantic compatibility issue
fastapi>=0.103.0,<0.105.0
pydantic>=2.4.0,<2.6.0
uvicorn[standard]>=0.23.0
python-multipart>=0.0.6

# ============ CONFIGURATION ============
hydra-core>=1.3.0
omegaconf>=2.3.0

# ============ TRANSLATION CACHING & PERSISTENCE ============
# НОВЫЙ РАЗДЕЛ: Зависимости для кэширования переводов

# JSON processing для кэша переводов
orjson>=3.9.0          # Быстрый JSON для кэша (альтернатива стандартному json)

# Hashing для cache keys
# (hashlib входит в стандартную библиотеку Python)

# File watching для hot-reload конфигурации
watchdog>=3.0.0        # Для мониторинга изменений конфигурационных файлов

# ============ AI MODEL SPECIFIC ============
# Segment Anything Model (SAM) для сегментации
# segment-anything>=1.0

# Для depth estimation (MiDaS)
# timm>=0.9.0

# Для edge detection и preprocessing
scipy>=1.11.0

# ============ GPU MONITORING & OPTIMIZATION ============
# GPU мониторинг и статистика
gpustat>=1.1.0
pynvml>=11.5.0  # NVIDIA Management Library для детального мониторинга

# Memory profiling
psutil>=5.9.0
py3nvml>=0.2.7  # Альтернативная библиотека для NVIDIA

# ============ OPTIMIZATION LIBRARIES ============
# Triton для custom CUDA kernels (если нужно)
# triton>=2.1.0

# Torch Audio для дополнительных возможностей
# torchaudio>=2.0.0

# ============ UTILITIES & HELPERS ============
# Для прогресс-баров при загрузке моделей
tqdm>=4.65.0

# Для работы с изображениями
Pillow>=10.0.0
# pillow-simd>=9.0.0  # Оптимизированная версия Pillow

# ============ ASYNC & CONCURRENCY ============
# НОВЫЙ РАЗДЕЛ: Для асинхронной работы Translation Service
asyncio-throttle>=1.0.2  # Rate limiting для API вызовов
aiofiles>=23.1.0         # Асинхронная работа с файлами (для кэша)

# ============ TEXT PROCESSING ============
# НОВЫЙ РАЗДЕЛ: Обработка текста для translation

# Продвинутая обработка текста
ftfy>=6.1.1             # Исправление битого текста/кодировок
unidecode>=1.3.6        # Транслитерация для fallback
unicodedata2>=15.0.0    # Расширенная поддержка Unicode

# Preprocessing текста
nltk>=3.8.1             # Natural Language Toolkit (для токенизации, если нужно)
# spacy>=3.6.0          # Более тяжелая альтернатива (закомментировано)

# ============ BASIC DEPENDENCIES ============
numpy>=1.24.0
requests>=2.31.0

# ============ DEVELOPMENT & DEBUGGING ============
# Для отладки GPU памяти
memory-profiler>=0.60.0

# Rich для красивого логирования
rich>=13.4.0            # Для enhanced console output

# ============ TESTING ============
pytest>=7.4.0
pytest-asyncio>=0.21.0

freetype-py
matplotlib

# ============ FORMAT SUPPORT ============
# Для работы с различными форматами изображений
# webp>=0.1.6
# avif>=0.1.0

# ============ TESTING (OPTIONAL) ============
# Для unit тестов AI компонентов
# pytest-gpu>=0.1.0  # Специализированные GPU тесты

# ============ EXPERIMENTAL (CUTTING EDGE) ============
# Для экспериментальных возможностей
# flash-attn>=2.0.0  # Flash Attention (может требовать специальную сборку)

# ============ TRANSLATION MODEL COMPATIBILITY NOTES ============
# ВАЖНО: Совместимость с различными translation моделями:
#
# Qwen модели:
# - Требуют transformers>=4.37.0
# - Используют trust_remote_code=True
# - Поддерживают torch.float16 для экономии памяти
# - Совместимы с CUDA 11.8+
#
# NLLB модели:
# - Требуют sentencepiece для токенизации
# - Используют специальные language codes (rus_Cyrl, eng_Latn, zho_Hans)
# - Размеры: 600M, 1.3B, 3.3B параметров
# - Хорошо работают с batch inference
#
# mT5 модели:
# - Стандартные transformers зависимости
# - Используют T5 архитектуру с prefix tuning
# - Размеры: small, base, large, xl, xxl
# - Поддерживают text-to-text generation
#
# OPUS модели:
# - Легкие и быстрые (Helsinki-NLP)
# - Специализированные пары языков
# - Минимальные requirements
# - Хорошо для fallback scenarios

# ============ MEMORY REQUIREMENTS FOR TRANSLATION ============
# Дополнительные требования памяти для translation моделей:
#
# Qwen2-7B:     ~14GB VRAM (float16)
# Qwen2-1.5B:   ~3GB VRAM (float16)
# NLLB-3.3B:    ~6.6GB VRAM (float16)
# NLLB-1.3B:    ~2.6GB VRAM (float16)
# mT5-XL:       ~7.4GB VRAM (float16)
# mT5-Large:    ~2.4GB VRAM (float16)
# OPUS models:  ~0.3GB VRAM (очень легкие)
#
# Общая память для полной системы (SDXL + ControlNet + Translation):
# 80GB VRAM: SDXL(~6GB) + Union ControlNet(~2GB) + Qwen2-7B(~14GB) = ~22GB + overhead
# 40GB VRAM: SDXL(~6GB) + Union ControlNet(~2GB) + NLLB-3.3B(~7GB) = ~15GB + overhead
# 24GB VRAM: SDXL(~6GB) + Union ControlNet(~2GB) + NLLB-1.3B(~3GB) = ~11GB + overhead

# ============ INSTALLATION ORDER FOR TRANSLATION ============
# Рекомендуемый порядок установки с translation support:
# 1. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
# 2. pip install transformers accelerate
# 3. pip install sentencepiece protobuf  # Для NLLB
# 4. pip install -r requirements_gpu.txt
# 5. Первый запуск загрузит модели автоматически (может занять время)

# ============ KNOWN ISSUES & WORKAROUNDS ============
# 1. XFormers может требовать CUDA 11.8+
# 2. controlnet-aux может конфликтовать с некоторыми версиями transformers
# 3. Qwen модели требуют trust_remote_code=True (проверьте источник)
# 4. NLLB модели могут быть медленными без оптимизации
# 5. mT5 модели требуют много памяти для больших размеров
# 6. sentencepiece может требовать специальной компиляции на некоторых системах

# ============ ENVIRONMENT VARIABLES ============
# Рекомендуемые переменные окружения для translation:
# export CUDA_VISIBLE_DEVICES=0
# export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
# export TOKENIZERS_PARALLELISM=false
# export TRANSFORMERS_CACHE=./cache/transformers
# export HF_HOME=./cache/huggingface
# export TRANSFORMERS_OFFLINE=0  # Разрешить загрузку моделей

# ============ MODEL DOWNLOAD NOTES ============
# При первом запуске будут загружены модели:
# - Stable Diffusion XL (~12GB)
# - Union ControlNet (~4GB) 
# - Translation model (зависит от выбранной, 0.3GB - 14GB)
# 
# Убедитесь что есть достаточно места на диске (~30GB+ свободного места)
# Модели кэшируются в ~/.cache/huggingface или в TRANSFORMERS_CACHE

# ============ COMPATIBILITY MATRIX ============
# Python 3.8  ✓ (minimum)
# Python 3.9  ✓ (recommended)  
# Python 3.10 ✓ (recommended)
# Python 3.11 ✓ (should work)
# Python 3.12 ⚠ (may have compatibility issues with some deps)
#
# CUDA 11.8   ✓ (recommended)
# CUDA 12.0   ✓ (should work)
# CUDA 12.1+  ⚠ (check PyTorch compatibility)
#
# GPU Memory:
# 12-16GB     ✓ (OPUS + lightweight models)
# 24GB        ✓ (NLLB-1.3B, mT5-Large)  
# 40GB        ✓ (NLLB-3.3B, mT5-XL)
# 80GB+       ✓ (Qwen2-7B, all models)
