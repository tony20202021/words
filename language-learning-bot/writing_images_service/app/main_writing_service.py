#!/usr/bin/env python
"""
Entry point for the writing image generation service with AI capabilities.
This module initializes and runs the FastAPI application for generating writing images using real AI models.
"""

import logging
import argparse
import os
import sys
import time
import signal
import asyncio
from pathlib import Path
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional

import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

# Add the parent directory to sys.path to allow imports from other modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from hydra import compose, initialize
from hydra.core.global_hydra import GlobalHydra

from app.api.routes import writing_images, health
from app.utils.logger import setup_logger
from app.utils import config_holder
from app.models.gpu_manager import GPUManager
from app.services.writing_image_service import WritingImageService

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
gpu_manager: Optional[GPUManager] = None
writing_service: Optional[WritingImageService] = None

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Hydra (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
try:
    # –û—á–∏—â–∞–µ–º GlobalHydra –µ—Å–ª–∏ –æ–Ω —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
    if GlobalHydra().is_initialized():
        GlobalHydra.instance().clear()
    
    # Initialize Hydra configuration
    initialize(config_path="../conf/config", version_base=None)
    cfg = compose(config_name="default")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ config_holder –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    config_holder.cfg = cfg
    
    print("‚úÖ Hydra configuration loaded successfully")
    
except Exception as e:
    print(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Hydra: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
    print("  - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è conf/config —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    print("  - –§–∞–π–ª conf/config/default.yaml —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    print("  - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
    sys.exit(1)

# Ensure logs directory exists
log_dir = cfg.logging.log_dir if hasattr(cfg, "logging") and hasattr(cfg.logging, "log_dir") else "logs"
os.makedirs(os.path.join(os.getcwd(), log_dir), exist_ok=True)

# Set up logging
log_level = cfg.logging.level if hasattr(cfg, "logging") and hasattr(cfg.logging, "level") else "INFO"
log_format = cfg.logging.format if hasattr(cfg, "logging") and hasattr(cfg.logging, "format") else "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"

logger = setup_logger(__name__, log_level=log_level, log_format=log_format, log_dir=log_dir)

async def _check_system_requirements():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
    logger.info("üîç Checking system requirements...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –≤–µ—Ä—Å–∏—é
    python_version = sys.version_info
    if python_version < (3, 8):
        raise RuntimeError(f"Python 3.8+ required, got {python_version.major}.{python_version.minor}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
    cuda_available = torch.cuda.is_available()
    if not cuda_available:
        raise RuntimeError("CUDA not available. GPU is required for AI image generation.")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
    device_count = torch.cuda.device_count()
    device_name = torch.cuda.get_device_name(0)
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    logger.info(f"‚úÖ System requirements check passed:")
    logger.info(f"   üêç Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    logger.info(f"   üéÆ CUDA: {torch.version.cuda}")
    logger.info(f"   üî• PyTorch: {torch.__version__}")
    logger.info(f"   üñ•Ô∏è GPU: {device_name}")
    logger.info(f"   üíæ GPU Memory: {total_memory:.1f}GB")
    logger.info(f"   üî¢ GPU Count: {device_count}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    critical_libraries = ["transformers", "diffusers", "accelerate", "safetensors"]
    missing_libraries = []
    
    for lib_name in critical_libraries:
        try:
            lib = __import__(lib_name)
            version = getattr(lib, "__version__", "unknown")
            logger.info(f"   üìö {lib_name}: {version}")
        except ImportError:
            missing_libraries.append(lib_name)
    
    if missing_libraries:
        raise RuntimeError(f"Critical AI libraries missing: {', '.join(missing_libraries)}")

async def _initialize_gpu_manager():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç GPU Manager."""
    global gpu_manager
    
    logger.info("üéÆ Initializing GPU Manager...")
    
    try:
        gpu_manager = GPUManager()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å GPU
        gpu_status = gpu_manager.get_gpu_status()
        
        logger.info(f"‚úÖ GPU Manager initialized:")
        logger.info(f"   üìä Memory: {gpu_status.used_memory_gb:.1f}GB / {gpu_status.total_memory_gb:.1f}GB")
        logger.info(f"   üå°Ô∏è Temperature: {gpu_status.temperature_celsius}¬∞C" if gpu_status.temperature_celsius else "   üå°Ô∏è Temperature: N/A")
        logger.info(f"   ‚ö° Power: {gpu_status.power_usage_watts}W" if gpu_status.power_usage_watts else "   ‚ö° Power: N/A")
        logger.info(f"   üèóÔ∏è Optimization Profile: {gpu_manager.optimization_profile.name}")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize GPU Manager: {e}")
        raise

async def _initialize_writing_service():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Writing Service (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ AI –º–æ–¥–µ–ª–µ–π)."""
    global writing_service
    
    logger.info("ü§ñ Initializing Writing Service...")
    
    try:
        writing_service = WritingImageService()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞
        service_status = await writing_service.get_service_status()
        
        logger.info(f"‚úÖ Writing Service initialized:")
        logger.info(f"   üîß Implementation: {service_status.get('implementation', 'unknown')}")
        logger.info(f"   üìä Generation count: {service_status.get('total_generations', 0)}")
        logger.info(f"   üèóÔ∏è AI Status: {service_status.get('ai_status', {}).get('initialized', 'not_initialized')}")
        logger.info("   ‚è≥ AI models will be loaded on first request (lazy initialization)")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Writing Service: {e}")
        raise

def _setup_signal_handlers():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown."""
    def signal_handler(signum, frame):
        logger.info(f"üì° Received signal {signum}, initiating graceful shutdown...")
        # uvicorn –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å shutdown —á–µ—Ä–µ–∑ lifespan
        
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    logger.info("üì° Signal handlers configured")

async def _log_service_configuration():
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–µ—Ä–≤–∏—Å–∞."""
    logger.info("‚öôÔ∏è Service Configuration:")
    
    # API –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    logger.info(f"   üåê API Host: {cfg.api.host}")
    logger.info(f"   üîå API Port: {cfg.api.port}")
    logger.info(f"   üîÑ Debug Mode: {cfg.api.debug}")
    logger.info(f"   üîó API Prefix: {cfg.api.prefix}")
    
    # AI –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    if hasattr(cfg, 'ai_generation'):
        ai_cfg = cfg.ai_generation
        if hasattr(ai_cfg, 'models'):
            logger.info(f"   ü§ñ Base Model: {ai_cfg.models.base_model}")
        if hasattr(ai_cfg, 'generation'):
            logger.info(f"   üé® Inference Steps: {ai_cfg.generation.num_inference_steps}")
            logger.info(f"   üéØ Guidance Scale: {ai_cfg.generation.guidance_scale}")
    
    # GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    if gpu_manager:
        memory_info = gpu_manager.get_memory_usage()
        logger.info(f"   üíæ GPU Memory Available: {memory_info.get('free_gb', 0):.1f}GB")
        logger.info(f"   üì¶ Recommended Batch Size: {gpu_manager.get_recommended_batch_size()}")

async def _cleanup_services():
    """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã —Å–µ—Ä–≤–∏—Å–æ–≤."""
    global writing_service, gpu_manager
    
    logger.info("üßπ Cleaning up services...")
    
    try:
        # –û—á–∏—â–∞–µ–º Writing Service
        if writing_service:
            await writing_service.cleanup()
            writing_service = None
            logger.info("‚úÖ Writing Service cleaned up")
        
        # –û—á–∏—â–∞–µ–º GPU Manager
        if gpu_manager:
            gpu_manager.clear_cache(aggressive=True)
            gpu_manager = None
            logger.info("‚úÖ GPU Manager cleaned up")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("‚úÖ GPU cache cleared")
        
    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan event handler –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—É—Å–∫–æ–º –∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π —Å–µ—Ä–≤–∏—Å–∞.
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ.
    """
    
    # ============ STARTUP ============
    startup_start = time.time()
    pid = os.getpid()
    parent_pid = os.getppid()
    
    logger.info("=" * 60)
    logger.info("üöÄ WRITING IMAGE SERVICE WITH AI GENERATION")
    logger.info("=" * 60)
    logger.info(f"üÜî Process ID (PID): {pid}")
    logger.info(f"üë®‚Äçüë¶ Parent PID: {parent_pid}")
    logger.info(f"üìÅ Working directory: {os.getcwd()}")
    logger.info(f"üêç Python: {sys.version}")
    logger.info(f"üíª Platform: {sys.platform}")
    logger.info(f"üîß Config source: Hydra")
    
    try:
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        await _check_system_requirements()
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GPU Manager
        await _initialize_gpu_manager()
        
        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Writing Service (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ AI –º–æ–¥–µ–ª–µ–π)
        await _initialize_writing_service()
        
        # 4. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∏–≥–Ω–∞–ª–æ–≤
        _setup_signal_handlers()
        
        # 5. –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        await _log_service_configuration()
        
        startup_time = time.time() - startup_start
        
        logger.info("=" * 60)
        logger.info(f"‚úÖ SERVICE STARTED SUCCESSFULLY in {startup_time:.2f}s")
        logger.info(f"üåê API docs: http://{cfg.api.host}:{cfg.api.port}{cfg.api.prefix}/docs")
        logger.info(f"‚ù§Ô∏è Health check: http://{cfg.api.host}:{cfg.api.port}/health")
        logger.info(f"ü§ñ AI models will load on first request")
        logger.info("=" * 60)
        
        yield  # –°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç
        
    except Exception as e:
        logger.error(f"‚ùå FAILED TO START SERVICE: {e}")
        logger.error("Service will not be available for requests")
        raise
    
    # ============ SHUTDOWN ============
    shutdown_start = time.time()
    logger.info("üõë SHUTTING DOWN Writing Image Service...")
    
    try:
        # –û—á–∏—â–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
        await _cleanup_services()
        
        shutdown_time = time.time() - shutdown_start
        logger.info(f"‚úÖ SERVICE SHUT DOWN GRACEFULLY in {shutdown_time:.2f}s")
        
    except Exception as e:
        logger.error(f"‚ùå Error during shutdown: {e}")

def create_application() -> FastAPI:
    """Create and configure the FastAPI application."""
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ cfg (Hydra –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω)
    api_prefix = cfg.api.prefix
    app_name = cfg.app.name
    app_environment = cfg.app.environment
    cors_origins = cfg.api.cors_origins
    
    # –ü–∞—Ä—Å–∏–º CORS origins
    if isinstance(cors_origins, str):
        cors_origins = cors_origins.split(",") if cors_origins != "*" else ["*"]
    
    app = FastAPI(
        title=f"{app_name} (AI Generation)",
        description="Service for generating writing images using real AI models for language learning",
        version="1.0.0",
        docs_url=f"{api_prefix}/docs",
        redoc_url=f"{api_prefix}/redoc",
        openapi_url=f"{api_prefix}/openapi.json",
        lifespan=lifespan  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π lifespan –≤–º–µ—Å—Ç–æ on_event
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π exception handler
    @app.exception_handler(Exception)
    async def global_exception_handler(request, exc):
        logger.error(f"Unhandled exception: {exc}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "detail": str(exc) if cfg.api.debug else "An unexpected error occurred"
            }
        )
    
    # Include routers
    app.include_router(writing_images.router, prefix=api_prefix)
    app.include_router(health.router)  # Health checks –Ω–∞ –∫–æ—Ä–Ω–µ–≤–æ–º —É—Ä–æ–≤–Ω–µ
    
    return app

app = create_application()

def run_server(port_override=None):
    """Run the server using uvicorn."""
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ cfg (Hydra –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω)
    host = cfg.api.host
    port = int(cfg.api.port)
    debug_mode = bool(cfg.api.debug)
    
    # Override port if specified
    if port_override:
        port = port_override
        logger.info(f"Port overridden to: {port}")
    
    if debug_mode:
        # Development mode with specific reload directories
        logger.info("üîÑ Starting in DEVELOPMENT mode with auto-reload")
        uvicorn.run(
            "app.main_writing_service:app",
            host=host,
            port=port,
            reload=True,
            reload_dirs=["app"], 
            log_level="info"
            # –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã *.py
            # –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ "conf" - –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ç—å —Å–µ—Ä–≤–∏—Å —Ä—É–∫–∞–º–∏
        )
    else:
        # Production mode without reload
        logger.info("üè≠ Starting in PRODUCTION mode")
        uvicorn.run(
            "app.main_writing_service:app",
            host=host,
            port=port,
            reload=False,
            log_level="info",
            workers=1  # AI –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç multi-worker
        )

if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser(description='–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å AI')
        parser.add_argument('--process-name', type=str, help='–ò–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏')
        parser.add_argument('--port', type=int, help='–ü–æ—Ä—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞')
        
        args, unknown = parser.parse_known_args()
        
        # –ö—Ä–∞—Ç–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if args.process_name:
            logger.info(f"üè∑Ô∏è Process name: {args.process_name}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä (–¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –±—É–¥–µ—Ç –≤ startup event)
        run_server(args.port)
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Service interrupted by user")
    except Exception as e:
        logger.error(f"üí• Critical error starting service: {e}", exc_info=True)
    finally:
        logger.info("üèÅ Service stopped")
